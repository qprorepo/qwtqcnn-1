{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac39a38-9667-40b7-b542-6ec4b6fe2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations (Resized to 1024x1024 for higher resolution)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1024, 1024)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (1024, 1024))  # Resize to 1024x1024\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (1024, 1024))  # Resize to 1024x1024\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define Quantum-Inspired Convolution Kernels (Simulating Quantum Gates)\n",
    "def quantum_kernel(theta):\n",
    "    return np.array([\n",
    "        [np.exp(1j * theta), 0, np.exp(-1j * theta)],\n",
    "        [0, -1, 0],\n",
    "        [np.exp(-1j * theta), 0, np.exp(1j * theta)]\n",
    "    ])\n",
    "\n",
    "# Adaptive stride function based on local image intensity\n",
    "def adaptive_stride(image, x, y):\n",
    "    local_intensity = np.mean(image[y:y+8, x:x+8])  # Calculate local intensity in an 8x8 window\n",
    "    if local_intensity > 0.7:  # High intensity -> smaller stride\n",
    "        return 1\n",
    "    elif local_intensity > 0.3:  # Medium intensity -> medium stride\n",
    "        return 2\n",
    "    else:  # Low intensity -> larger stride\n",
    "        return 4\n",
    "\n",
    "# Quantum convolution with adaptive stride and dynamic quantum gates\n",
    "def quantum_convolution_adaptive(image, theta):\n",
    "    h, w = image.shape\n",
    "    output = np.zeros_like(image)\n",
    "\n",
    "    for y in range(0, h, 8):  # Initial step size\n",
    "        for x in range(0, w, 8):\n",
    "            stride = adaptive_stride(image, x, y)  # Get adaptive stride\n",
    "            kernel = quantum_kernel(theta)  # Apply quantum gate with angle theta\n",
    "            \n",
    "            # Apply convolution with quantum gate (complex kernel)\n",
    "            convolved_real = cv2.filter2D(image[y:y+8, x:x+8], -1, kernel.real)\n",
    "            convolved_imag = cv2.filter2D(image[y:y+8, x:x+8], -1, kernel.imag)\n",
    "            convolved = np.sqrt(convolved_real**2 + convolved_imag**2)  # Quantum measurement\n",
    "            \n",
    "            # Normalize the convolved output to maintain consistent brightness\n",
    "            convolved = (convolved - convolved.min()) / (convolved.max() - convolved.min() + 1e-8)\n",
    "            \n",
    "            # Place the convolved result into the output image using adaptive stride\n",
    "            output[y:y+8:stride, x:x+8:stride] = convolved[::stride, ::stride]\n",
    "\n",
    "    return output\n",
    "\n",
    "# Dynamic Quantum Gates (Different theta values)\n",
    "thetas = [np.pi/4, np.pi/2, 10*np.pi/4]\n",
    "\n",
    "# Process all images with Quantum Convolution and Adaptive Stride\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "plt.figure(figsize=(25, 30), dpi=100)  # Increase figure size for high-resolution images\n",
    "\n",
    "for idx, image in enumerate(images):\n",
    "    convolved_images = [quantum_convolution_adaptive(image, theta) for theta in thetas]\n",
    "    \n",
    "    for i, convolved_image in enumerate(convolved_images):\n",
    "        ax = plt.subplot(4, len(thetas), idx*len(thetas) + i + 1)\n",
    "        plt.imshow(convolved_image, cmap='inferno')  # Use inferno colormap for visualization\n",
    "        plt.title(f'{titles[idx]} - Quantum Convolution (Î¸={thetas[i]:.2f})', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add border to subplot for differentiation\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('black')\n",
    "            spine.set_linewidth(2)\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15d26c-597b-4d0d-8832-9b62f4d91047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load MNIST dataset with increased resolution\n",
    "transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset with increased resolution\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image and resize to 512x512\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (512, 512))\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image and resize to 512x512\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (512, 512))\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "\n",
    "pauli_x = np.array([[0, 1], [1, 0]])  # Pauli-X gate (bit-flip)\n",
    "pauli_z = np.array([[1, 0], [0, -1]])  # Pauli-Z gate (phase-flip)\n",
    "hadamard = (1/np.sqrt(2)) * np.array([[1, 1], [1, -1]])  # Hadamard gate\n",
    "\n",
    "\n",
    "def apply_quantum_gate(image, gate):\n",
    "    h, w = image.shape\n",
    "    output = np.zeros_like(image, dtype=np.complex128)\n",
    "    \n",
    "    for y in range(0, h, 2):\n",
    "        for x in range(0, w, 2):\n",
    "            pixel_block = image[y:y+2, x:x+2]  # 2x2 block representing a quantum state\n",
    "            if pixel_block.shape == (2, 2):\n",
    "                quantum_state = np.array([[pixel_block[0, 0], pixel_block[0, 1]],\n",
    "                                          [pixel_block[1, 0], pixel_block[1, 1]]])\n",
    "                transformed_state = gate @ quantum_state @ gate.T  # Apply the quantum gate\n",
    "                output[y:y+2, x:x+2] = transformed_state\n",
    "    \n",
    "    return np.abs(output)  # Return magnitude to simulate measurement\n",
    "\n",
    "# Simulate Quantum Entanglement by correlating pixel values in different regions\n",
    "def apply_quantum_entanglement(image):\n",
    "    h, w = image.shape\n",
    "    output = np.copy(image)\n",
    "    \n",
    "    for y in range(0, h, 4):\n",
    "        for x in range(0, w, 4):\n",
    "            entangled_block = image[y:y+4, x:x+4]  # 4x4 block to simulate entanglement\n",
    "            if entangled_block.shape == (4, 4):\n",
    "                average_value = np.mean(entangled_block)\n",
    "                output[y:y+4, x:x+4] = average_value  # Correlate all pixels in the block\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Process images with different quantum operations\n",
    "def process_with_quantum_operations(images, titles):\n",
    "    plt.figure(figsize=(30, 30))  # Increase figure size for high-resolution images\n",
    "    \n",
    "    for idx, image in enumerate(images):\n",
    "        # Apply Quantum Gates\n",
    "        transformed_image_x = apply_quantum_gate(image, pauli_x)\n",
    "        transformed_image_z = apply_quantum_gate(image, pauli_z)\n",
    "        transformed_image_h = apply_quantum_gate(image, hadamard)\n",
    "        \n",
    "        # Apply Quantum Entanglement\n",
    "        entangled_image = apply_quantum_entanglement(image)\n",
    "        \n",
    "        # Visualization\n",
    "        for col, (transformed_image, operation) in enumerate(zip(\n",
    "            [transformed_image_x, transformed_image_z, transformed_image_h, entangled_image],\n",
    "            ['Pauli-X', 'Pauli-Z', 'Hadamard', 'Entanglement']\n",
    "        )):\n",
    "            ax = plt.subplot(len(images), 4, idx * 4 + col + 1)\n",
    "            plt.imshow(transformed_image, cmap='inferno')\n",
    "            plt.title(f'{titles[idx]} - {operation}', fontsize=24)\n",
    "            plt.axis('off')\n",
    "            ax.set_facecolor('xkcd:light grey')  # Highlight each subplot background with light grey\n",
    "            for spine in ax.spines.values():  # Add a border around each subplot\n",
    "                spine.set_edgecolor('white')\n",
    "                spine.set_linewidth(2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Images to process\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "# Process and visualize with quantum operations\n",
    "process_with_quantum_operations(images, titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0424a-07d3-4a19-a008-c197bc90f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (256, 256))  # Increase resolution\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (256, 256))  # Increase resolution\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define Quantum-Inspired Complex Quantum Gates (Pauli-X, Pauli-Z, etc.)\n",
    "pauli_x = np.array([[0, 1], [1, 0]])  # Pauli-X gate (bit-flip)\n",
    "pauli_z = np.array([[1, 0], [0, -1]])  # Pauli-Z gate (phase-flip)\n",
    "hadamard = (1/np.sqrt(2)) * np.array([[1, 1], [1, -1]])  # Hadamard gate\n",
    "\n",
    "# Apply Quantum-Inspired Operations with Complex Quantum Gates\n",
    "def apply_quantum_gate(image, gate):\n",
    "    h, w = image.shape\n",
    "    output = np.zeros_like(image, dtype=np.complex128)\n",
    "    \n",
    "    for y in range(0, h, 2):\n",
    "        for x in range(0, w, 2):\n",
    "            pixel_block = image[y:y+2, x:x+2]  # 2x2 block representing a quantum state\n",
    "            if pixel_block.shape == (2, 2):\n",
    "                quantum_state = np.array([[pixel_block[0, 0], pixel_block[0, 1]],\n",
    "                                          [pixel_block[1, 0], pixel_block[1, 1]]])\n",
    "                transformed_state = gate @ quantum_state @ gate.T  # Apply the quantum gate\n",
    "                output[y:y+2, x:x+2] = transformed_state\n",
    "    \n",
    "    return np.abs(output)  # Return magnitude to simulate measurement\n",
    "\n",
    "# Simulate Quantum Entanglement by correlating pixel values in different regions\n",
    "def apply_quantum_entanglement(image):\n",
    "    h, w = image.shape\n",
    "    output = np.copy(image)\n",
    "    \n",
    "    for y in range(0, h, 4):\n",
    "        for x in range(0, w, 4):\n",
    "            entangled_block = image[y:y+4, x:x+4]  # 4x4 block to simulate entanglement\n",
    "            if entangled_block.shape == (4, 4):\n",
    "                average_value = np.mean(entangled_block)\n",
    "                output[y:y+4, x:x+4] = average_value  # Correlate all pixels in the block\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Process images with different quantum operations\n",
    "def process_with_quantum_operations(images, titles):\n",
    "    plt.figure(figsize=(20, 20))  # Increase figure size for high-resolution images\n",
    "    \n",
    "    for idx, image in enumerate(images):\n",
    "        # Apply Quantum Gates\n",
    "        transformed_image_x = apply_quantum_gate(image, pauli_x)\n",
    "        transformed_image_z = apply_quantum_gate(image, pauli_z)\n",
    "        transformed_image_h = apply_quantum_gate(image, hadamard)\n",
    "        \n",
    "        # Apply Quantum Entanglement\n",
    "        entangled_image = apply_quantum_entanglement(image)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.subplot(len(images), 4, idx * 4 + 1)\n",
    "        plt.imshow(transformed_image_x, cmap='inferno')\n",
    "        plt.title(f'{titles[idx]} - Pauli-X')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(len(images), 4, idx * 4 + 2)\n",
    "        plt.imshow(transformed_image_z, cmap='inferno')\n",
    "        plt.title(f'{titles[idx]} - Pauli-Z')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(len(images), 4, idx * 4 + 3)\n",
    "        plt.imshow(transformed_image_h, cmap='inferno')\n",
    "        plt.title(f'{titles[idx]} - Hadamard')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(len(images), 4, idx * 4 + 4)\n",
    "        plt.imshow(entangled_image, cmap='inferno')\n",
    "        plt.title(f'{titles[idx]} - Entanglement')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Images to process\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "# Process and visualize with quantum operations\n",
    "process_with_quantum_operations(images, titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0db971-9970-4cbc-8371-27275efd1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load MNIST dataset with increased resolution\n",
    "transform = transforms.Compose([transforms.Resize((512, 512)), transforms.ToTensor()])\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset with increased resolution\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image and resize to 512x512\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (512, 512))\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image and resize to 512x512\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (512, 512))\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define Quantum-Inspired Convolution Kernels with Adaptive Behavior\n",
    "complex_kernels = [\n",
    "    np.array([[0, 1j, 0], [1j, -1, 1j], [0, 1j, 0]]),  # Complex Sharpen Kernel with imaginary parts\n",
    "    np.array([[1+1j, 1-1j, 1+1j], [1-1j, -8+1j, 1-1j], [1+1j, 1-1j, 1+1j]]),  # Complex Edge Detection\n",
    "    np.array([[1+0j, 2+1j, 1+0j], [0+1j, 0+0j, 0-1j], [-1+0j, -2-1j, -1+0j]]),  # Complex Sobel X Kernel\n",
    "]\n",
    "\n",
    "# Adaptive stride function based on local image intensity\n",
    "def adaptive_stride(image, x, y):\n",
    "    local_intensity = np.mean(image[y:y+8, x:x+8])  # Calculate local intensity in an 8x8 window\n",
    "    if local_intensity > 0.7:  # High intensity -> smaller stride\n",
    "        return 1\n",
    "    elif local_intensity > 0.3:  # Medium intensity -> medium stride\n",
    "        return 2\n",
    "    else:  # Low intensity -> larger stride\n",
    "        return 3  # Smaller maximum stride to maintain pixel visibility\n",
    "\n",
    "# Apply Quantum Convolution with Adaptive Stride and Dynamic Quantum Gates\n",
    "def quantum_convolution_adaptive(image, kernels):\n",
    "    h, w = image.shape\n",
    "    output = np.zeros_like(image)\n",
    "\n",
    "    for y in range(0, h, 8):  # Initial step size\n",
    "        for x in range(0, w, 8):\n",
    "            stride = adaptive_stride(image, x, y)  # Get adaptive stride\n",
    "            kernel_idx = (x + y) % len(kernels)  # Select kernel dynamically based on position\n",
    "            kernel = kernels[kernel_idx]\n",
    "            \n",
    "            # Apply convolution with dynamic quantum gate (kernel)\n",
    "            convolved_real = cv2.filter2D(image[y:y+8, x:x+8], -1, kernel.real)\n",
    "            convolved_imag = cv2.filter2D(image[y:y+8, x:x+8], -1, kernel.imag)\n",
    "            convolved = np.sqrt(convolved_real**2 + convolved_imag**2)\n",
    "            \n",
    "            # Place the convolved result into the output image using adaptive stride\n",
    "            output[y:y+8:stride, x:x+8:stride] = convolved[::stride, ::stride]\n",
    "\n",
    "    return output\n",
    "\n",
    "# Process all images with Quantum Convolution and Adaptive Stride\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "# Visualize the results with all subplots in one row\n",
    "plt.figure(figsize=(40, 10))  # Adjust figure size for a single row of subplots\n",
    "for idx, image in enumerate(images):\n",
    "    convolved_image = quantum_convolution_adaptive(image, complex_kernels)\n",
    "    plt.subplot(1, 4, idx + 1)  # Use 1 row and 4 columns\n",
    "    plt.imshow(convolved_image, cmap='inferno')  # Use inferno colormap for visualization\n",
    "    plt.title(f'{titles[idx]} - Adaptive Quantum Convolution', fontsize=24)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7cb47-c0c4-4d9b-884a-440868718591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (256, 256))  # Resize to a standard size\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (256, 256))  # Resize to a standard size\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define Quantum-Inspired Convolution Kernels with Adaptive Behavior\n",
    "complex_kernels = [\n",
    "    np.array([[0, 1j, 0], [1j, -1, 1j], [0, 1j, 0]]),  # Complex Sharpen Kernel with imaginary parts\n",
    "    np.array([[1+1j, 1-1j, 1+1j], [1-1j, -8+1j, 1-1j], [1+1j, 1-1j, 1+1j]]),  # Complex Edge Detection\n",
    "    np.array([[1+0j, 2+1j, 1+0j], [0+1j, 0+0j, 0-1j], [-1+0j, -2-1j, -1+0j]]),  # Complex Sobel X Kernel\n",
    "]\n",
    "\n",
    "# Adaptive stride function based on local image intensity\n",
    "def adaptive_stride(image, x, y):\n",
    "    local_intensity = np.mean(image[y:y+8, x:x+8])  # Calculate local intensity in an 8x8 window\n",
    "    if local_intensity > 0.7:  # High intensity -> smaller stride\n",
    "        return 1\n",
    "    elif local_intensity > 0.3:  # Medium intensity -> medium stride\n",
    "        return 2\n",
    "    else:  # Low intensity -> larger stride\n",
    "        return 4\n",
    "\n",
    "# Apply Quantum Convolution with Adaptive Stride and Dynamic Quantum Gates\n",
    "def quantum_convolution_adaptive(image, kernels):\n",
    "    h, w = image.shape\n",
    "    output = np.zeros_like(image)\n",
    "\n",
    "    for y in range(0, h, 8):  # Initial step size\n",
    "        for x in range(0, w, 8):\n",
    "            stride = adaptive_stride(image, x, y)  # Get adaptive stride\n",
    "            kernel_idx = (x + y) % len(kernels)  # Select kernel dynamically based on position\n",
    "            kernel = kernels[kernel_idx]\n",
    "            \n",
    "            # Apply convolution with dynamic quantum gate (kernel)\n",
    "            convolved_real = cv2.filter2D(image[y:y+8, x:x+8], -1, kernel.real)\n",
    "            convolved_imag = cv2.filter2D(image[y:y+8, x:x+8], -1, kernel.imag)\n",
    "            convolved = np.sqrt(convolved_real**2 + convolved_imag**2)\n",
    "            \n",
    "            # Place the convolved result into the output image using adaptive stride\n",
    "            output[y:y+8:stride, x:x+8:stride] = convolved[::stride, ::stride]\n",
    "\n",
    "    return output\n",
    "\n",
    "# Process all images with Quantum Convolution and Adaptive Stride\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "# Visualize the results with all subplots in one row\n",
    "plt.figure(figsize=(40, 10))  # Adjust figure size for a single row of subplots\n",
    "for idx, image in enumerate(images):\n",
    "    convolved_image = quantum_convolution_adaptive(image, complex_kernels)\n",
    "    plt.subplot(1, 4, idx + 1)  # Use 1 row and 4 columns\n",
    "    plt.imshow(convolved_image, cmap='inferno')  # Use inferno colormap for visualization\n",
    "    plt.title(f'{titles[idx]} - Adaptive Quantum Convolution', fontsize=24)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1a6e7-f11e-4696-8721-74bd029868ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (256, 256))  # Increase resolution\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (256, 256))  # Increase resolution\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define Complex Quantum-Inspired Convolution Kernels\n",
    "complex_kernels = [\n",
    "    np.array([[0, 1j, 0], [1j, -1, 1j], [0, 1j, 0]]),  # Complex Sharpen Kernel with imaginary parts\n",
    "    np.array([[1+1j, 1-1j, 1+1j], [1-1j, -8+1j, 1-1j], [1+1j, 1-1j, 1+1j]]),  # Complex Edge Detection\n",
    "    np.array([[1+0j, 2+1j, 1+0j], [0+1j, 0+0j, 0-1j], [-1+0j, -2-1j, -1+0j]]),  # Complex Sobel X Kernel\n",
    "    np.array([[1+1j, 0-1j, -1+1j], [2-2j, 0+0j, -2+2j], [1-1j, 0+1j, -1-1j]]),  # Complex Sobel Y Kernel\n",
    "]\n",
    "\n",
    "# Apply complex quantum-inspired convolution and pooling\n",
    "def quantum_complex_convolution(image, kernels):\n",
    "    convolved_images = []\n",
    "    for kernel in kernels:\n",
    "        convolved_real = cv2.filter2D(image, -1, kernel.real)  # Apply real part of the kernel\n",
    "        convolved_imag = cv2.filter2D(image, -1, kernel.imag)  # Apply imaginary part of the kernel\n",
    "        convolved = np.sqrt(convolved_real**2 + convolved_imag**2)  # Calculate magnitude\n",
    "        convolved = (convolved - np.min(convolved)) / (np.max(convolved) - np.min(convolved))  # Normalize\n",
    "        pooled = cv2.resize(convolved, (128, 128))  # Simulate max pooling by resizing to 128x128\n",
    "        convolved_images.append(pooled)\n",
    "    return convolved_images\n",
    "\n",
    "# Process all images\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "# Optimize and Visualize with 'inferno' colormap\n",
    "plt.figure(figsize=(20, 20))  # Increase figure size for high-resolution images\n",
    "for idx, image in enumerate(images):\n",
    "    convolved_images = quantum_complex_convolution(image, complex_kernels)\n",
    "    for k_idx, convolved_image in enumerate(convolved_images):\n",
    "        ax = plt.subplot(len(images), len(complex_kernels), idx * len(complex_kernels) + k_idx + 1)\n",
    "        plt.imshow(convolved_image, cmap='inferno')  # Use inferno colormap for better visualization\n",
    "        plt.title(f'{titles[idx]} - Kernel {k_idx + 1}', fontsize=12)  # Highlight titles\n",
    "        plt.axis('off')\n",
    "        ax.set_facecolor('xkcd:light grey')  # Set background to light grey for differentiation\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885ae63-064d-41b3-acb0-ff2649f349d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Transformation for MNIST and FashionMNIST datasets\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = mnist_image.numpy()[0]  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = fashion_image.numpy()[0]  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (256, 256))\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (256, 256))\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define Optimized Convolution Kernels (Extended)\n",
    "kernels = [\n",
    "    np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),  # Sharpen Kernel\n",
    "    np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),  # Edge Detection\n",
    "    np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]),  # Sobel X Kernel\n",
    "    np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]]),  # Sobel Y Kernel\n",
    "    np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]]),  # Laplacian Kernel\n",
    "]\n",
    "\n",
    "# Apply convolution and pooling with adjusted pooling method\n",
    "def quantum_convolution(image, kernels):\n",
    "    convolved_images = []\n",
    "    for kernel in kernels:\n",
    "        convolved = cv2.filter2D(image, -1, kernel)\n",
    "        pooled = F.max_pool2d(torch.tensor(convolved).unsqueeze(0).unsqueeze(0), kernel_size=2).squeeze().numpy()\n",
    "        convolved_images.append(pooled)\n",
    "    return convolved_images\n",
    "\n",
    "# Process all images\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "# Optimize and Visualize\n",
    "plt.figure(figsize=(20, 18))  # Increased figure size for better readability\n",
    "for idx, image in enumerate(images):\n",
    "    convolved_images = quantum_convolution(image, kernels)\n",
    "    for k_idx, convolved_image in enumerate(convolved_images):\n",
    "        ax = plt.subplot(len(images), len(kernels), idx * len(kernels) + k_idx + 1)\n",
    "        plt.imshow(convolved_image, cmap='inferno')\n",
    "        plt.title(f'{titles[idx]} - Kernel {k_idx + 1}', fontsize=10)  # Highlight titles\n",
    "        plt.axis('off')\n",
    "        ax.set_facecolor('xkcd:light grey')  # Highlight each subplot background\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c6f65-feb5-49a8-87d1-63120e8320f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_images():\n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    mnist_image, _ = mnist_trainset[0]\n",
    "    q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "    # Load FashionMNIST dataset\n",
    "    fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "    fashion_image, _ = fashion_trainset[0]\n",
    "    q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "    # Load Pepper image\n",
    "    download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "    download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if download_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "    download_image = cv2.resize(download_image, (256, 256))  # Increase resolution\n",
    "    q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Load Liver image\n",
    "    liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "    liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if liver_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "    liver_image = cv2.resize(liver_image, (256, 256))  # Increase resolution\n",
    "    q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    return q_mnist_image, q_fashion_image, q_download_image, q_liver_image\n",
    "\n",
    "# Quantum-inspired convolution operation\n",
    "def quantum_convolution(image, kernel):\n",
    "    \"\"\"Simulate quantum convolution by applying a kernel to an image.\"\"\"\n",
    "    convolved_image = cv2.filter2D(image, -1, kernel)\n",
    "    return convolved_image\n",
    "\n",
    "# Quantum-inspired pooling operation (Max Pooling)\n",
    "def quantum_pooling(image, size=2):\n",
    "    \"\"\"Simulate quantum pooling by applying max pooling to an image.\"\"\"\n",
    "    pooled_image = cv2.resize(image, (image.shape[0] // size, image.shape[1] // size))\n",
    "    return pooled_image\n",
    "\n",
    "# Highlight the processed images\n",
    "def highlight_image(image, border_color=(255, 0, 0), border_thickness=5):\n",
    "    \"\"\"Highlight the image by adding a border.\"\"\"\n",
    "    if len(image.shape) == 2:  # Grayscale image\n",
    "        image = (image * 255).astype(np.uint8)  # Convert to uint8\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    highlighted_image = cv2.copyMakeBorder(\n",
    "        image, border_thickness, border_thickness, border_thickness, border_thickness,\n",
    "        cv2.BORDER_CONSTANT, value=border_color\n",
    "    )\n",
    "    return highlighted_image\n",
    "\n",
    "# Main visualization function\n",
    "def visualize_datasets():\n",
    "    # Load and preprocess images\n",
    "    q_mnist_image, q_fashion_image, q_download_image, q_liver_image = load_and_preprocess_images()\n",
    "\n",
    "    # Define Quantum-like Convolution Kernels\n",
    "    kernels = {\n",
    "        'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n",
    "        'Edge Detection': np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),\n",
    "        'Sobel X': np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    }\n",
    "\n",
    "    images = {\n",
    "        'MNIST': q_mnist_image,\n",
    "        'FashionMNIST': q_fashion_image,\n",
    "        'Pepper': q_download_image,\n",
    "        'Liver': q_liver_image\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(len(images), len(kernels) + 2, figsize=(20, 20))\n",
    "\n",
    "    for idx, (image_name, image) in enumerate(images.items()):\n",
    "        # Display the original image\n",
    "        axes[idx, 0].imshow(cv2.resize(image, (256, 256)), cmap='gray')\n",
    "        axes[idx, 0].set_title(f\"Original {image_name} Image\")\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        for jdx, (kernel_name, kernel) in enumerate(kernels.items()):\n",
    "            # Apply convolution and pooling\n",
    "            convolved_image = quantum_convolution(image, kernel)\n",
    "            pooled_image = quantum_pooling(convolved_image)\n",
    "\n",
    "            # Highlight and display the convolved image\n",
    "            highlighted_convolved = highlight_image(cv2.resize(convolved_image, (256, 256)), border_color=(255, 0, 0))\n",
    "            axes[idx, jdx + 1].imshow(highlighted_convolved)\n",
    "            axes[idx, jdx + 1].set_title(f\"{kernel_name} Convolution\")\n",
    "            axes[idx, jdx + 1].axis('off')\n",
    "\n",
    "            # Highlight and display the pooled image\n",
    "            highlighted_pooled = highlight_image(cv2.resize(pooled_image, (256, 256)), border_color=(0, 0, 255))\n",
    "            axes[idx, jdx + 2].imshow(highlighted_pooled)\n",
    "            axes[idx, jdx + 2].set_title(f\"{kernel_name} Pooling\")\n",
    "            axes[idx, jdx + 2].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "visualize_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc3596-93d2-4d3f-b578-0476a9f7ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_images():\n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    mnist_image, _ = mnist_trainset[0]\n",
    "    q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "    # Load FashionMNIST dataset\n",
    "    fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "    fashion_image, _ = fashion_trainset[0]\n",
    "    q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "    # Load Pepper image\n",
    "    download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "    download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if download_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "    download_image = cv2.resize(download_image, (256, 256), interpolation=cv2.INTER_NEAREST)  # Increase resolution\n",
    "    q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Load Liver image\n",
    "    liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "    liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if liver_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "    liver_image = cv2.resize(liver_image, (256, 256), interpolation=cv2.INTER_NEAREST)  # Increase resolution\n",
    "    q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    return q_mnist_image, q_fashion_image, q_download_image, q_liver_image\n",
    "\n",
    "# Quantum-inspired convolution operation\n",
    "def quantum_convolution(image, kernel):\n",
    "    \"\"\"Simulate quantum convolution by applying a kernel to an image.\"\"\"\n",
    "    convolved_image = cv2.filter2D(image, -1, kernel)\n",
    "    return convolved_image\n",
    "\n",
    "# Quantum-inspired pooling operation (Max Pooling)\n",
    "def quantum_pooling(image, size=2):\n",
    "    \"\"\"Simulate quantum pooling by applying max pooling to an image.\"\"\"\n",
    "    pooled_image = image[::size, ::size]\n",
    "    return pooled_image\n",
    "\n",
    "# Highlight the processed images\n",
    "def highlight_image(image, border_color=(255, 0, 0), border_thickness=5):\n",
    "    \"\"\"Highlight the image by adding a border.\"\"\"\n",
    "    if len(image.shape) == 2:  # Grayscale image\n",
    "        image = (image * 255).astype(np.uint8)  # Convert to uint8\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    highlighted_image = cv2.copyMakeBorder(\n",
    "        image, border_thickness, border_thickness, border_thickness, border_thickness,\n",
    "        cv2.BORDER_CONSTANT, value=border_color\n",
    "    )\n",
    "    return highlighted_image\n",
    "\n",
    "# Main visualization function\n",
    "def visualize_datasets():\n",
    "    # Load and preprocess images\n",
    "    q_mnist_image, q_fashion_image, q_download_image, q_liver_image = load_and_preprocess_images()\n",
    "\n",
    "    # Define Quantum-like Convolution Kernels\n",
    "    kernels = {\n",
    "        'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n",
    "        'Edge Detection': np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),\n",
    "        'Sobel X': np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    }\n",
    "\n",
    "    images = {\n",
    "        'MNIST': q_mnist_image,\n",
    "        'FashionMNIST': q_fashion_image,\n",
    "        'Pepper': q_download_image,\n",
    "        'Liver': q_liver_image\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(len(images), len(kernels) * 2 + 1, figsize=(20, 12))\n",
    "\n",
    "    for idx, (image_name, image) in enumerate(images.items()):\n",
    "        # Display the original image\n",
    "        axes[idx, 0].imshow(cv2.resize(image, (256, 256), interpolation=cv2.INTER_NEAREST), cmap='gray')\n",
    "        axes[idx, 0].set_title(f\"Original {image_name} Image\")\n",
    "        axes[idx, 0].axis('off')\n",
    "\n",
    "        for jdx, (kernel_name, kernel) in enumerate(kernels.items()):\n",
    "            # Apply convolution and pooling\n",
    "            convolved_image = quantum_convolution(image, kernel)\n",
    "            pooled_image = quantum_pooling(convolved_image)\n",
    "\n",
    "            # Highlight and display the convolved image\n",
    "            highlighted_convolved = highlight_image(cv2.resize(convolved_image, (256, 256), interpolation=cv2.INTER_NEAREST), border_color=(255, 0, 0))\n",
    "            axes[idx, jdx * 2 + 1].imshow(highlighted_convolved)\n",
    "            axes[idx, jdx * 2 + 1].set_title(f\"{kernel_name} Convolution\")\n",
    "            axes[idx, jdx * 2 + 1].axis('off')\n",
    "\n",
    "            # Highlight and display the pooled image\n",
    "            highlighted_pooled = highlight_image(cv2.resize(pooled_image, (256, 256), interpolation=cv2.INTER_NEAREST), border_color=(0, 0, 255))\n",
    "            axes[idx, jdx * 2 + 2].imshow(highlighted_pooled)\n",
    "            axes[idx, jdx * 2 + 2].set_title(f\"{kernel_name} Pooling\")\n",
    "            axes[idx, jdx * 2 + 2].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "visualize_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f71b03-2a12-4e1d-bdba-613c169fa083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.ndimage import maximum_filter\n",
    "\n",
    "# Quantum-inspired convolution operation\n",
    "def quantum_convolution(image, kernel):\n",
    "    \"\"\"Simulate quantum convolution by applying a kernel to an image.\"\"\"\n",
    "    convolved_image = cv2.filter2D(image, -1, kernel)\n",
    "    return convolved_image\n",
    "\n",
    "# Quantum-inspired pooling operation (Max Pooling)\n",
    "def quantum_pooling(image, size=2):\n",
    "    \"\"\"Simulate quantum pooling by applying max pooling to an image.\"\"\"\n",
    "    pooled_image = maximum_filter(image, size=size)\n",
    "    return pooled_image\n",
    "\n",
    "# Define multiple quantum-inspired convolution kernels\n",
    "kernels = {\n",
    "    'Identity': np.array([[0, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 0, 0]], dtype=float),\n",
    "    \n",
    "    'Entanglement': np.array([[0.25, 0.5, 0.25],\n",
    "                              [0.5, 1, 0.5],\n",
    "                              [0.25, 0.5, 0.25]], dtype=float),\n",
    "    \n",
    "    'Superposition': np.array([[1, -1, 1],\n",
    "                               [-1, 1, -1],\n",
    "                               [1, -1, 1]], dtype=float),\n",
    "    \n",
    "    'Phase Shift': np.array([[0, 1, 0],\n",
    "                             [1, -1, 1],\n",
    "                             [0, 1, 0]], dtype=float),\n",
    "    \n",
    "    'QFT': np.array([[0.5, 0.5, 0.5],\n",
    "                                           [0.5, -0.5, 0.5],\n",
    "                                           [0.5, 0.5, -0.5]], dtype=float)\n",
    "}\n",
    "\n",
    "# Load MNIST image\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])\n",
    "\n",
    "# Load and preprocess FashionMNIST image with higher resolution\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "fashion_image = cv2.resize(np.abs(fashion_image.numpy()[0]), (224, 224), interpolation=cv2.INTER_NEAREST)  # Higher resolution\n",
    "q_fashion_image = fashion_image / 255.0\n",
    "\n",
    "# Load and preprocess downloaded image with higher resolution\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (224, 224), interpolation=cv2.INTER_NEAREST)  # Higher resolution\n",
    "q_download_image = download_image / 255.0\n",
    "\n",
    "# Load and preprocess liver image with higher resolution\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (224, 224), interpolation=cv2.INTER_NEAREST)  # Higher resolution\n",
    "q_liver_image = liver_image / 255.0\n",
    "\n",
    "# Perform quantum-inspired convolutions and pooling\n",
    "images = {\n",
    "    'MNIST': q_mnist_image,\n",
    "    'FashionMNIST': q_fashion_image,\n",
    "    'Pepper': q_download_image,\n",
    "    'Liver': q_liver_image\n",
    "}\n",
    "\n",
    "# Create subplots to display results\n",
    "fig, axes = plt.subplots(len(images), len(kernels) * 2 + 1, figsize=(20, 8))\n",
    "\n",
    "# Minimize font size and reduce spacing\n",
    "title_fontsize = 8  # Set title font size to a smaller value\n",
    "plt.subplots_adjust(hspace=0.1, wspace=0.1)  # Adjust space between subplots\n",
    "\n",
    "for i, (image_name, image) in enumerate(images.items()):\n",
    "    # Display the original image\n",
    "    axes[i, 0].imshow(image, cmap='gray')\n",
    "    axes[i, 0].set_title(f\"Original {image_name} Image\", fontsize=title_fontsize)\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    for j, (kernel_name, kernel) in enumerate(kernels.items()):\n",
    "        convolved_image = quantum_convolution(image, kernel)\n",
    "        pooled_image = quantum_pooling(convolved_image, size=2)\n",
    "        \n",
    "        # Convolved image\n",
    "        axes[i, j * 2 + 1].imshow(convolved_image, cmap='gray')\n",
    "        axes[i, j * 2 + 1].set_title(f\"{kernel_name} Convolution\", fontsize=title_fontsize)\n",
    "        axes[i, j * 2 + 1].axis('off')\n",
    "        \n",
    "        # Pooled image\n",
    "        axes[i, j * 2 + 2].imshow(pooled_image, cmap='gray')\n",
    "        axes[i, j * 2 + 2].set_title(f\"{kernel_name} Pooling\", fontsize=title_fontsize)\n",
    "        axes[i, j * 2 + 2].axis('off')\n",
    "\n",
    "plt.tight_layout(pad=0.5)  # Further tighten layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f94983-bd07-468a-a261-fd6b05ffb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Quantum-inspired convolution operation\n",
    "def quantum_convolution(image, kernel):\n",
    "    \"\"\"Simulate quantum convolution by applying a kernel to an image.\"\"\"\n",
    "    convolved_image = cv2.filter2D(image, -1, kernel)\n",
    "    return convolved_image\n",
    "\n",
    "# Define quantum-inspired convolution kernels\n",
    "kernel_identity = np.array([[0, 0, 0],\n",
    "                            [0, 1, 0],\n",
    "                            [0, 0, 0]], dtype=float)\n",
    "\n",
    "kernel_entanglement = np.array([[0.25, 0.5, 0.25],\n",
    "                                [0.5, 1, 0.5],\n",
    "                                [0.25, 0.5, 0.25]], dtype=float)\n",
    "\n",
    "kernel_superposition = np.array([[1, -1, 1],\n",
    "                                 [-1, 1, -1],\n",
    "                                 [1, -1, 1]], dtype=float)\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_image_datasets():\n",
    "    # Load MNIST image\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    mnist_image, _ = mnist_trainset[0]\n",
    "    mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "    # Load FashionMNIST image\n",
    "    fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "    fashion_image, _ = fashion_trainset[0]\n",
    "    fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "    # Load and preprocess Pepper image\n",
    "    download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "    download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if download_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "    download_image = cv2.resize(download_image, (28, 28))\n",
    "    download_image = download_image / 255.0\n",
    "\n",
    "    # Load and preprocess liver image\n",
    "    liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "    liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if liver_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "    liver_image = cv2.resize(liver_image, (28, 28))\n",
    "    liver_image = liver_image / 255.0\n",
    "\n",
    "    return {\n",
    "        'MNIST': mnist_image,\n",
    "        'FashionMNIST': fashion_image,\n",
    "        'Pepper': download_image,\n",
    "        'Liver': liver_image\n",
    "    }\n",
    "\n",
    "# Function to highlight changes in processed images\n",
    "def highlight_changes(original, processed):\n",
    "    \"\"\"Highlight the changes between the original and processed images.\"\"\"\n",
    "    difference = np.abs(processed - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))\n",
    "    highlighter[..., 0] = difference  # Red for changes\n",
    "    highlighter[..., 1] = original  # Green for the original image\n",
    "    highlighter[..., 2] = original  # Blue for the original image\n",
    "    return highlighter\n",
    "\n",
    "# Main function to visualize quantum convolutions\n",
    "def visualize_convolutions():\n",
    "    images = load_image_datasets()\n",
    "\n",
    "    # Define the convolutions to apply\n",
    "    convolutions = {\n",
    "        'Identity': kernel_identity,\n",
    "        'Entanglement': kernel_entanglement,\n",
    "        'Superposition': kernel_superposition\n",
    "    }\n",
    "\n",
    "    # Create subplots to display results\n",
    "    fig, axes = plt.subplots(len(images), len(convolutions) + 1, figsize=(20, 15))\n",
    "    fig.suptitle('Quantum Convolution Visualization with Highlights', fontsize=16)\n",
    "\n",
    "    for i, (image_name, image) in enumerate(images.items()):\n",
    "        axes[i, 0].imshow(image, cmap='gray')\n",
    "        axes[i, 0].set_title(f\"Original {image_name} Image\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        for j, (conv_name, kernel) in enumerate(convolutions.items()):\n",
    "            convolved_image = quantum_convolution(image, kernel)\n",
    "            highlighted_image = highlight_changes(image, convolved_image)\n",
    "            axes[i, j + 1].imshow(highlighted_image)\n",
    "            axes[i, j + 1].set_title(f\"{conv_name} Convolution\")\n",
    "            axes[i, j + 1].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "visualize_convolutions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e14fc5-d655-42b6-b192-13c5842ff19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_images():\n",
    "    # Load MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    image, _ = trainset[0]\n",
    "    q_image = np.abs(image.numpy()[0])  # Extract the first MNIST image\n",
    "    q_image = cv2.resize(q_image, (1024, 1024))  # Increase resolution to 1024x1024\n",
    "\n",
    "    # Load Fashion-MNIST image\n",
    "    trainset_fashion = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "    image_fashion, _ = trainset_fashion[0]\n",
    "    q_fashion_image = np.abs(image_fashion.numpy()[0])  # Extract the first Fashion-MNIST image\n",
    "    q_fashion_image = cv2.resize(q_fashion_image, (1024, 1024))  # Increase resolution to 1024x1024\n",
    "\n",
    "    # Load Pepper image\n",
    "    download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "    download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if download_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "    download_image = cv2.resize(download_image, (1024, 1024))  # Increase resolution to 1024x1024\n",
    "    q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Load liver image\n",
    "    liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "    liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if liver_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "    liver_image = cv2.resize(liver_image, (1024, 1024))  # Increase resolution to 1024x1024\n",
    "    q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    return q_image, q_fashion_image, q_download_image, q_liver_image\n",
    "\n",
    "# Quantum-inspired operations\n",
    "def depthwise_conv2d(image, kernel, stride=1):\n",
    "    \"\"\"Simulate depthwise convolution on a 2D image using a 2D kernel.\"\"\"\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    \n",
    "    output_height = (image_height - kernel_height) // stride + 1\n",
    "    output_width = (image_width - kernel_width) // stride + 1\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    for i in range(0, output_height, stride):\n",
    "        for j in range(0, output_width, stride):\n",
    "            region = image[i:i+kernel_height, j:j+kernel_width]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def batch_normalization(image, mean=0.0, std=1.0):\n",
    "    \"\"\"Simulate quantum batch normalization by normalizing image pixel values.\"\"\"\n",
    "    img_mean = np.mean(image)\n",
    "    img_std = np.std(image)\n",
    "    return (image - img_mean) / (img_std + 1e-8) * std + mean\n",
    "\n",
    "def max_pooling(image, pool_size=2, stride=2):\n",
    "    \"\"\"Simulate quantum pooling operation by reducing dimensionality.\"\"\"\n",
    "    image_height, image_width = image.shape\n",
    "    output_height = (image_height - pool_size) // stride + 1\n",
    "    output_width = (image_width - pool_size) // stride + 1\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    for i in range(0, output_height, stride):\n",
    "        for j in range(0, output_width, stride):\n",
    "            region = image[i:i+pool_size, j:j+pool_size]\n",
    "            output[i, j] = np.max(region)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def quantum_fourier_transform(image):\n",
    "    \"\"\"Simulate quantum Fourier transform (QFT) using NumPy's FFT.\"\"\"\n",
    "    return np.fft.fft2(image)\n",
    "\n",
    "def inverse_quantum_fourier_transform(image):\n",
    "    \"\"\"Simulate inverse quantum Fourier transform using NumPy's IFFT.\"\"\"\n",
    "    return np.fft.ifft2(image).real\n",
    "\n",
    "def noise_reduction(image):\n",
    "    \"\"\"Simulate quantum noise reduction (simplified).\"\"\"\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "def quantum_error_correction(image):\n",
    "    \"\"\"Simulate quantum error correction (simplified).\"\"\"\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "def highlight_changes(original, processed):\n",
    "    \"\"\"Highlight the changes between the original and processed images.\"\"\"\n",
    "    processed_resized = cv2.resize(processed, original.shape)  # Resize processed image to match original\n",
    "    difference = np.abs(processed_resized - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))\n",
    "    highlighter[..., 0] = difference  # Red for changes\n",
    "    highlighter[..., 1] = original  # Green for the original image\n",
    "    highlighter[..., 2] = original  # Blue for the original image\n",
    "    return highlighter\n",
    "\n",
    "def quantum_convolution_pipeline(image, kernel, stride=1, pool_size=2, bn_mean=0.0, bn_std=1.0):\n",
    "    \"\"\"Simulate full quantum convolution with all operations.\"\"\"\n",
    "    # Step 1: Depthwise Convolution\n",
    "    conv_image = depthwise_conv2d(image, kernel, stride)\n",
    "    \n",
    "    # Step 2: Quantum Batch Normalization with specified mean and std\n",
    "    qbn_image = batch_normalization(conv_image, bn_mean, bn_std)\n",
    "    \n",
    "    # Step 3: Quantum Fourier Transform (QFT)\n",
    "    qft_image = quantum_fourier_transform(qbn_image)\n",
    "    \n",
    "    # Step 4: Noise Reduction and Error Correction\n",
    "    nr_image = noise_reduction(qft_image)\n",
    "    qec_image = quantum_error_correction(nr_image)\n",
    "    \n",
    "    # Step 5: Inverse Quantum Fourier Transform\n",
    "    iqft_image = inverse_quantum_fourier_transform(qec_image)\n",
    "    \n",
    "    # Normalize Inverse QFT using magnitude normalization\n",
    "    iqft_image = np.abs(iqft_image) / np.max(np.abs(iqft_image))\n",
    "\n",
    "    # Step 6: Pooling\n",
    "    pooled_image = max_pooling(iqft_image, pool_size)\n",
    "    \n",
    "    return conv_image, qbn_image, np.abs(qft_image), np.abs(nr_image), iqft_image, pooled_image\n",
    "\n",
    "# Visualizing different datasets with varying batch normalization parameters\n",
    "def visualize_datasets():\n",
    "    q_image, q_fashion_image, q_download_image, q_liver_image = load_and_preprocess_images()\n",
    "    kernel = np.array([[1, -1], [-1, 1]])  # Simple kernel for testing\n",
    "\n",
    "    datasets = {'MNIST': q_image, 'Fashion-MNIST': q_fashion_image, 'Pepper': q_download_image, 'Liver': q_liver_image}\n",
    "    bn_means = [10.0, 20.0, 30.0]\n",
    "    bn_stds = [0.5, 1.0, 1.5]\n",
    "\n",
    "    fig, axes = plt.subplots(len(datasets), 5, figsize=(30, 20))  # Adjust figure size for better resolution\n",
    "\n",
    "    for idx, (dataset_name, dataset_image) in enumerate(datasets.items()):\n",
    "        for i, (bn_mean, bn_std) in enumerate(zip(bn_means, bn_stds)):\n",
    "            conv_image, qbn_image, qft_image, nr_image, iqft_image, pooled_image = quantum_convolution_pipeline(\n",
    "                dataset_image, kernel, bn_mean=bn_mean, bn_std=bn_std\n",
    "            )\n",
    "            \n",
    "            # Display each step in the pipeline for the current dataset\n",
    "            if i == 0:\n",
    "                axes[idx, 0].imshow(cv2.resize(conv_image, (1024, 1024)), cmap='gray')\n",
    "                axes[idx, 0].set_title(f'{dataset_name} Convolution', fontsize=14)\n",
    "                axes[idx, 1].imshow(cv2.resize(qbn_image, (1024, 1024)), cmap='gray')\n",
    "                axes[idx, 1].set_title('Batch Norm (Mean 10, Std 0.5)', fontsize=14)\n",
    "            elif i == 1:\n",
    "                axes[idx, 2].imshow(cv2.resize(qbn_image, (1024, 1024)), cmap='gray')\n",
    "                axes[idx, 2].set_title('Batch Norm (Mean 20, Std 1.0)', fontsize=14)\n",
    "            else:\n",
    "                axes[idx, 3].imshow(cv2.resize(qbn_image, (1024, 1024)), cmap='gray')\n",
    "                axes[idx, 3].set_title('Batch Norm (Mean 30, Std 1.5)', fontsize=14)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "visualize_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058bb67-16b8-4d91-9007-54165bdb08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_and_preprocess_images():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "    image, _ = trainset[0]\n",
    "    q_image = np.abs(image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "    # Load Fashion-MNIST image\n",
    "    trainset_fashion = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "    image_fashion, _ = trainset_fashion[0]\n",
    "    q_fashion_image = np.abs(image_fashion.numpy()[0])  # Extract the first Fashion-MNIST image\n",
    "\n",
    "    # Load Pepper image\n",
    "    download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "    download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if download_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "    download_image = cv2.resize(download_image, (28, 28), interpolation=cv2.INTER_NEAREST)  # Preserve pixel clarity\n",
    "    q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Load liver image\n",
    "    liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "    liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if liver_image is None:\n",
    "        raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "    liver_image = cv2.resize(liver_image, (28, 28), interpolation=cv2.INTER_NEAREST)  # Preserve pixel clarity\n",
    "    q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    return q_image, q_fashion_image, q_download_image, q_liver_image\n",
    "\n",
    "# Quantum-inspired operations\n",
    "def depthwise_conv2d(image, kernel, stride=1):\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    output_height = (image_height - kernel_height) // stride + 1\n",
    "    output_width = (image_width - kernel_width) // stride + 1\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    for i in range(0, output_height, stride):\n",
    "        for j in range(0, output_width, stride):\n",
    "            region = image[i:i+kernel_height, j:j+kernel_width]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def batch_normalization(image, mean=0.0, std=1.0):\n",
    "    img_mean = np.mean(image)\n",
    "    img_std = np.std(image)\n",
    "    return (image - img_mean) / (img_std + 1e-8) * std + mean\n",
    "\n",
    "def l2_pooling(image, pool_size=2, stride=2):\n",
    "    image_height, image_width = image.shape\n",
    "    output_height = (image_height - pool_size) // stride + 1\n",
    "    output_width = (image_width - pool_size) // stride + 1\n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    for i in range(0, output_height, stride):\n",
    "        for j in range(0, output_width, stride):\n",
    "            region = image[i:i+pool_size, j:j+pool_size]\n",
    "            output[i, j] = np.sqrt(np.mean(region ** 2))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def quantum_fourier_transform(image):\n",
    "    return np.fft.fft2(image)\n",
    "\n",
    "def inverse_quantum_fourier_transform(image):\n",
    "    return np.fft.ifft2(image).real\n",
    "\n",
    "def noise_reduction(image):\n",
    "    real_image = np.abs(image)\n",
    "    filtered_image = cv2.GaussianBlur(real_image, (5, 5), 0)\n",
    "    return np.clip(filtered_image, 0, 1)\n",
    "\n",
    "def quantum_error_correction(image):\n",
    "    corrected_image = np.clip(image, 0, 1)\n",
    "    corrected_image = cv2.medianBlur(corrected_image.astype(np.float32), 3)\n",
    "    return corrected_image\n",
    "\n",
    "def highlight_changes(original, processed):\n",
    "    processed_resized = cv2.resize(processed, original.shape, interpolation=cv2.INTER_NEAREST)  # Ensure pixel clarity\n",
    "    difference = np.abs(processed_resized - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))\n",
    "    highlighter[..., 0] = difference  # Red for changes\n",
    "    highlighter[..., 1] = original  # Green for the original image\n",
    "    highlighter[..., 2] = original  # Blue for the original image\n",
    "    return highlighter\n",
    "\n",
    "def quantum_convolution_pipeline(image, kernel, stride=1, pool_size=2, bn_mean=0.0, bn_std=3.0):\n",
    "    conv_image = depthwise_conv2d(image, kernel, stride)\n",
    "    qbn_image = batch_normalization(conv_image, bn_mean, bn_std)\n",
    "    qft_image = quantum_fourier_transform(qbn_image)\n",
    "    nr_image = noise_reduction(qft_image)\n",
    "    qec_image = quantum_error_correction(nr_image)\n",
    "    iqft_image = inverse_quantum_fourier_transform(qec_image)\n",
    "    iqft_image = np.abs(iqft_image) / np.max(np.abs(iqft_image))\n",
    "    pooled_image = l2_pooling(iqft_image, pool_size)\n",
    "\n",
    "    return conv_image, qbn_image, np.abs(qft_image), np.abs(nr_image), iqft_image, pooled_image\n",
    "\n",
    "# Visualizing different datasets with varying batch normalization parameters\n",
    "def visualize_datasets():\n",
    "    q_image, q_fashion_image, q_download_image, q_liver_image = load_and_preprocess_images()\n",
    "    kernel = np.array([[1, -1], [-1, 1]])  # Simple kernel for testing\n",
    "\n",
    "    datasets = {'MNIST': q_image, 'Fashion-MNIST': q_fashion_image, 'Pepper': q_download_image, 'Liver': q_liver_image}\n",
    "    bn_means = [0.0, 1.0, 2.0]  # Increase mean values\n",
    "    bn_stds = [2.0, 3.0, 4.0]   # Increase standard deviation values\n",
    "\n",
    "    fig, axes = plt.subplots(len(datasets), 4, figsize=(20, 20))\n",
    "\n",
    "    for idx, (dataset_name, dataset_image) in enumerate(datasets.items()):\n",
    "        for i, (bn_mean, bn_std) in enumerate(zip(bn_means, bn_stds)):\n",
    "            conv_image, qbn_image, qft_image, nr_image, iqft_image, pooled_image = quantum_convolution_pipeline(\n",
    "                dataset_image, kernel, bn_mean=bn_mean, bn_std=bn_std\n",
    "            )\n",
    "            \n",
    "            # Display each step in the pipeline for the current dataset\n",
    "            if i == 0:\n",
    "                axes[idx, 0].imshow(cv2.resize(dataset_image, (256, 256), interpolation=cv2.INTER_NEAREST), cmap='gray')\n",
    "                axes[idx, 0].set_title(f'{dataset_name} Original')\n",
    "                axes[idx, 1].imshow(cv2.resize(conv_image, (256, 256), interpolation=cv2.INTER_NEAREST), cmap='gray')\n",
    "                axes[idx, 1].set_title('Convolution')\n",
    "                axes[idx, 2].imshow(cv2.resize(qbn_image, (256, 256), interpolation=cv2.INTER_NEAREST), cmap='gray')\n",
    "                axes[idx, 2].set_title(f'Batch Norm (Mean {bn_mean}, Std {bn_stds[0]})')\n",
    "\n",
    "            else:\n",
    "                highlighted_image = highlight_changes(dataset_image, pooled_image)\n",
    "                axes[idx, 3].imshow(cv2.resize(highlighted_image, (256, 256), interpolation=cv2.INTER_NEAREST), cmap='gray')\n",
    "                axes[idx, 3].set_title(f'L2 Pooling \\n(Mean {bn_mean}, Std {bn_stds[2]})')\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "visualize_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67cfbd2-5083-4fa6-a55b-a8659e64fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.fft import fft\n",
    "\n",
    "# Load datasets\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# MNIST dataset\n",
    "trainset_mnist = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "image_mnist, _ = trainset_mnist[0]\n",
    "q_image_mnist = np.abs(image_mnist.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Fashion-MNIST dataset (replaces Lenna image)\n",
    "trainset_fashion = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "image_fashion, _ = trainset_fashion[0]\n",
    "q_image_fashion = np.abs(image_fashion.numpy()[0])  # Extract the first Fashion-MNIST image\n",
    "\n",
    "# Pepper image\n",
    "pepper_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "pepper_image = cv2.imread(pepper_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if pepper_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {pepper_image_path}\")\n",
    "pepper_image = cv2.resize(pepper_image, (28, 28))\n",
    "q_pepper_image = pepper_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (28, 28))  # Resize to match the other images\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Helper function to highlight changes\n",
    "def highlight_changes(original, processed):\n",
    "    difference = np.abs(processed - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))  # Create a blank RGB image\n",
    "    highlighter[..., 0] = difference  # Red channel for highlighting\n",
    "    highlighter[..., 1] = original  # Green channel for the original image intensity\n",
    "    highlighter[..., 2] = original  # Blue channel for the original image intensity\n",
    "    return highlighter\n",
    "\n",
    "# Apply and visualize the process for each dataset\n",
    "datasets = {\n",
    "    \"MNIST\": q_image_mnist,\n",
    "    \"Fashion-MNIST\": q_image_fashion,\n",
    "    \"Pepper\": q_pepper_image,\n",
    "    \"Liver\": q_liver_image\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"Visualizing for {name} dataset...\")\n",
    "\n",
    "    # 1. Quantum Superposition Visualization\n",
    "    states = [\"Ï_kernel_0\", \"Ï_kernel_1\", \"Ï_kernel_2\"]\n",
    "    probabilities = np.random.rand(3) \n",
    "\n",
    "    plt.figure(figsize=(18, 4))\n",
    "    plt.subplot(1, 3, 1)  # Use 1x3 grid for plots\n",
    "    plt.bar(states, probabilities, color='purple', alpha=0.7)\n",
    "    plt.xlabel(\"Quantum States\")\n",
    "    plt.ylabel(\"Probability Amplitude\")\n",
    "    plt.title(f\"Quantum Superposition of Depthwise Kernels: {name}\")\n",
    "\n",
    "    # 2. Quantum Fourier Transform (QFT) Visualization\n",
    "    qft_result = np.abs(fft(data.flatten()))\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(qft_result, color='green')\n",
    "    plt.xlabel(\"Frequency Components\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.title(f\"Quantum Fourier Transform (QFT) Result: {name}\")\n",
    "\n",
    "    # 3. Quantum Convolution Visualization\n",
    "    q_conv_image = np.clip(data * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_conv_high_res = cv2.resize(q_conv_image, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    data_high_res = cv2.resize(data, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    highlighted_image = highlight_changes(data_high_res, q_conv_high_res)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(highlighted_image)\n",
    "    plt.title(f'Quantum Convolution: {name}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b1d3c-dabe-4a3a-a55c-6975c27da781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.fft import fft\n",
    "\n",
    "# Load datasets\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# MNIST dataset\n",
    "trainset_mnist = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "image_mnist, _ = trainset_mnist[0]\n",
    "q_image_mnist = np.abs(image_mnist.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Fashion-MNIST dataset\n",
    "trainset_fashion = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "image_fashion, _ = trainset_fashion[0]\n",
    "q_image_fashion = np.abs(image_fashion.numpy()[0])  # Extract the first Fashion-MNIST image\n",
    "\n",
    "# Pepper image\n",
    "pepper_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "pepper_image = cv2.imread(pepper_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if pepper_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {pepper_image_path}\")\n",
    "pepper_image = cv2.resize(pepper_image, (28, 28))\n",
    "q_pepper_image = pepper_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (28, 28))  # Resize to match the other images\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Helper function to highlight changes\n",
    "def highlight_changes(original, processed):\n",
    "    difference = np.abs(processed - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))  # Create a blank RGB image\n",
    "    highlighter[..., 0] = difference  # Red channel for highlighting\n",
    "    highlighter[..., 1] = original  # Green channel for the original image intensity\n",
    "    highlighter[..., 2] = original  # Blue channel for the original image intensity\n",
    "    return highlighter\n",
    "\n",
    "# Apply and visualize the process for each dataset\n",
    "datasets = {\n",
    "    \"MNIST\": q_image_mnist,\n",
    "    \"Fashion-MNIST\": q_image_fashion,\n",
    "    \"Pepper\": q_pepper_image,\n",
    "    \"Liver\": q_liver_image\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 12))  # Set the size of the complete graph\n",
    "\n",
    "for idx, (name, data) in enumerate(datasets.items(), start=1):\n",
    "    # 1. Quantum Superposition Visualization\n",
    "    states = [\"Ï_kernel_0\", \"Ï_kernel_1\", \"Ï_kernel_2\"]\n",
    "    probabilities = np.random.rand(3) \n",
    "\n",
    "    plt.subplot(len(datasets), 3, 3*idx-2)  # Position for Quantum Superposition\n",
    "    plt.bar(states, probabilities, color='purple', alpha=0.7)\n",
    "    plt.xlabel(\"Quantum States\")\n",
    "    plt.ylabel(\"Probability Amplitude\")\n",
    "    plt.title(f\"Quantum Superposition: {name}\")\n",
    "\n",
    "    # 2. Quantum Fourier Transform (QFT) Visualization\n",
    "    qft_result = np.abs(fft(data.flatten()))\n",
    "\n",
    "    plt.subplot(len(datasets), 3, 3*idx-1)  # Position for QFT\n",
    "    plt.plot(qft_result, color='green')\n",
    "    plt.xlabel(\"Frequency Components\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.title(f\"QFT Result: {name}\")\n",
    "\n",
    "    # 3. Quantum Convolution Visualization\n",
    "    q_conv_image = np.clip(data * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_conv_high_res = cv2.resize(q_conv_image, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    data_high_res = cv2.resize(data, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "    highlighted_image = highlight_changes(data_high_res, q_conv_high_res)\n",
    "\n",
    "    plt.subplot(len(datasets), 3, 3*idx)  # Position for Quantum Convolution\n",
    "    plt.imshow(highlighted_image)\n",
    "    plt.title(f'Quantum Convolution: {name}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea533b-75d2-460d-a95c-9379049bef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Quantum Fourier Transform (QFT) and its inverse\n",
    "def quantum_fourier_transform(q_image):\n",
    "    return np.fft.fft2(q_image)\n",
    "\n",
    "def inverse_quantum_fourier_transform(q_image):\n",
    "    return np.fft.ifft2(q_image)\n",
    "\n",
    "# Padding operation\n",
    "def pad_image(q_image, pad_width):\n",
    "    return np.pad(q_image, pad_width, mode='constant')\n",
    "\n",
    "# Stride operation (downsampling)\n",
    "def stride_operation(q_image, stride):\n",
    "    return q_image[::stride, ::stride]\n",
    "\n",
    "# Pooling operation (for simplicity, we'll use average pooling)\n",
    "def quantum_pooling(q_image, pool_size):\n",
    "    output_shape = (q_image.shape[0] // pool_size, q_image.shape[1] // pool_size)\n",
    "    pooled_image = np.zeros(output_shape)\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            pooled_image[i, j] = np.mean(q_image[i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size])\n",
    "    return pooled_image\n",
    "\n",
    "# Quantum Batch Normalization operation\n",
    "def quantum_batch_normalization(q_image, m_BN=0.9, epsilon_BN=1e-5):\n",
    "    mu = np.mean(np.abs(q_image))\n",
    "    sigma2 = np.var(np.abs(q_image))\n",
    "    q_image_normalized = (q_image - mu) / np.sqrt(sigma2 + epsilon_BN)\n",
    "    q_image_normalized *= m_BN\n",
    "    return q_image_normalized\n",
    "\n",
    "# Quantum convolution operation with stride (simplified for visualization)\n",
    "def quantum_convolution(q_image, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN):\n",
    "    # Padding\n",
    "    padded_image = pad_image(q_image, pad_width)\n",
    "    \n",
    "    # Stride operation (downsampling)\n",
    "    strided_image = stride_operation(padded_image, stride)\n",
    "    \n",
    "    # Quantum Fourier Transform\n",
    "    qft_image = quantum_fourier_transform(strided_image)\n",
    "    \n",
    "    # Adjust kernel size to match the strided image\n",
    "    if qft_image.shape[0] >= q_kernel.shape[0] and qft_image.shape[1] >= q_kernel.shape[1]:\n",
    "        kernel_padded = pad_image(q_kernel, ((qft_image.shape[0] - q_kernel.shape[0]) // 2, \n",
    "                                             (qft_image.shape[1] - q_kernel.shape[1]) // 2))\n",
    "    else:\n",
    "        kernel_padded = q_kernel[:qft_image.shape[0], :qft_image.shape[1]]\n",
    "    \n",
    "    # Convolution (element-wise multiplication in frequency domain)\n",
    "    conv_result = qft_image * kernel_padded\n",
    "    \n",
    "    # Inverse QFT\n",
    "    inv_qft_result = inverse_quantum_fourier_transform(conv_result)\n",
    "    \n",
    "    # Pooling\n",
    "    pooled_result = quantum_pooling(np.abs(inv_qft_result), pool_size)\n",
    "    \n",
    "    # Quantum Batch Normalization\n",
    "    normalized_result = quantum_batch_normalization(pooled_result, m_BN, epsilon_BN)\n",
    "    \n",
    "    return normalized_result\n",
    "\n",
    "# Helper function to highlight changes\n",
    "def highlight_changes(original, processed):\n",
    "    difference = np.abs(processed - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))  # Create a blank RGB image\n",
    "    highlighter[..., 0] = difference  # Red channel for highlighting differences\n",
    "    highlighter[..., 1] = original    # Green and Blue channels for original image intensity\n",
    "    highlighter[..., 2] = original\n",
    "    return highlighter\n",
    "\n",
    "# Load datasets\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# MNIST dataset\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "image, _ = trainset[0]\n",
    "q_image = np.abs(image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Fashion-MNIST dataset\n",
    "trainset_fashion = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "image_fashion, _ = trainset_fashion[0]\n",
    "q_image_fashion = np.abs(image_fashion.numpy()[0])  # Extract the first Fashion-MNIST image\n",
    "\n",
    "# Download image (Pepper image)\n",
    "download_image = cv2.imread('C:/Users/HP/Desktop/pepper.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise FileNotFoundError(\"The image file was not found. Please check the file path.\")\n",
    "download_image = cv2.resize(download_image, (28, 28))\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Parameters for quantum convolution\n",
    "pad_width = 2\n",
    "pool_size = 2\n",
    "m_BN = 0.9\n",
    "epsilon_BN = 1e-5\n",
    "\n",
    "# Different stride values to test\n",
    "stride_values = [6, 7, 8, 9, 10]  # Different stride sizes\n",
    "\n",
    "\n",
    "q_kernel = np.random.rand(28, 28)  # Same size as images\n",
    "\n",
    "# Plotting the results\n",
    "fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n",
    "\n",
    "# Apply quantum convolution to MNIST image with different stride values\n",
    "for i, stride in enumerate(stride_values):\n",
    "    q_conv_mnist = quantum_convolution(q_image, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN)\n",
    "    q_conv_mnist = np.clip(q_conv_mnist * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_high_res = resize(q_image, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    q_conv_high_res = resize(q_conv_mnist, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_mnist = highlight_changes(q_image_high_res, q_conv_high_res)\n",
    "    axs[0, i].imshow(highlighted_mnist, interpolation='nearest')  # Ensure pixel visibility\n",
    "    axs[0, i].set_title(f'MNIST Stride {stride}')\n",
    "    axs[0, i].axis('off')\n",
    "\n",
    "# Apply quantum convolution to Fashion-MNIST image with different stride values\n",
    "for i, stride in enumerate(stride_values):\n",
    "    q_conv_fashion = quantum_convolution(q_image_fashion, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN)\n",
    "    q_conv_fashion = np.clip(q_conv_fashion * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_fashion_high_res = resize(q_image_fashion, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    q_conv_fashion_high_res = resize(q_conv_fashion, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_fashion = highlight_changes(q_image_fashion_high_res, q_conv_fashion_high_res)\n",
    "    axs[1, i].imshow(highlighted_fashion, interpolation='nearest')  # Ensure pixel visibility\n",
    "    axs[1, i].set_title(f'Fashion-MNIST Stride {stride}')\n",
    "    axs[1, i].axis('off')\n",
    "\n",
    "# Apply quantum convolution to Download image with different stride values\n",
    "for i, stride in enumerate(stride_values):\n",
    "    q_conv_download = quantum_convolution(q_download_image, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN)\n",
    "    q_conv_download = np.clip(q_conv_download * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_download_high_res = resize(q_download_image, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    q_conv_download_high_res = resize(q_conv_download, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_download = highlight_changes(q_image_download_high_res, q_conv_download_high_res)\n",
    "    axs[2, i].imshow(highlighted_download, interpolation='nearest')  # Ensure pixel visibility\n",
    "    axs[2, i].set_title(f'Pepper Stride {stride}')\n",
    "    axs[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c20a1b-00d2-428a-a52b-10dff8752e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Quantum Fourier Transform (QFT) and its inverse\n",
    "def quantum_fourier_transform(q_image):\n",
    "    return np.fft.fft2(q_image)\n",
    "\n",
    "def inverse_quantum_fourier_transform(q_image):\n",
    "    return np.fft.ifft2(q_image)\n",
    "\n",
    "# Padding operation\n",
    "def pad_image(q_image, pad_width):\n",
    "    return np.pad(q_image, pad_width, mode='constant')\n",
    "\n",
    "# Stride operation (downsampling)\n",
    "def stride_operation(q_image, stride):\n",
    "    return q_image[::stride, ::stride]\n",
    "\n",
    "# Pooling operation (average pooling)\n",
    "def quantum_pooling(q_image, pool_size):\n",
    "    output_shape = (q_image.shape[0] // pool_size, q_image.shape[1] // pool_size)\n",
    "    pooled_image = np.zeros(output_shape)\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            pooled_image[i, j] = np.mean(q_image[i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size])\n",
    "    return pooled_image\n",
    "\n",
    "# Batch normalization operation\n",
    "def batch_normalization(q_image, m_BN=0.9, epsilon_BN=1e-5):\n",
    "    mu = np.mean(q_image)\n",
    "    sigma2 = np.var(q_image)\n",
    "    return (q_image - mu) / np.sqrt(sigma2 + epsilon_BN)\n",
    "\n",
    "# Quantum convolution operation with stride (simplified for visualization)\n",
    "def quantum_convolution(q_image, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN):\n",
    "    # Padding\n",
    "    padded_image = pad_image(q_image, pad_width)\n",
    "    \n",
    "    # Stride operation (downsampling)\n",
    "    strided_image = stride_operation(padded_image, stride)\n",
    "    \n",
    "    # Quantum Fourier Transform\n",
    "    qft_image = quantum_fourier_transform(strided_image)\n",
    "    \n",
    "    # Adjust kernel size to match the strided image\n",
    "    if qft_image.shape[0] >= q_kernel.shape[0] and qft_image.shape[1] >= q_kernel.shape[1]:\n",
    "        kernel_padded = pad_image(q_kernel, ((qft_image.shape[0] - q_kernel.shape[0]) // 2, \n",
    "                                             (qft_image.shape[1] - q_kernel.shape[1]) // 2))\n",
    "    else:\n",
    "        # Trim kernel if it's larger than the image\n",
    "        kernel_padded = q_kernel[:qft_image.shape[0], :qft_image.shape[1]]\n",
    "    \n",
    "    # Convolution (element-wise multiplication in frequency domain)\n",
    "    conv_result = qft_image * kernel_padded\n",
    "    \n",
    "    # Inverse QFT\n",
    "    inv_qft_result = inverse_quantum_fourier_transform(conv_result)\n",
    "    \n",
    "    # Pooling\n",
    "    pooled_result = quantum_pooling(np.abs(inv_qft_result), pool_size)\n",
    "    \n",
    "    # Batch Normalization\n",
    "    normalized_result = batch_normalization(pooled_result, m_BN, epsilon_BN)\n",
    "    \n",
    "    return normalized_result\n",
    "\n",
    "# Load datasets\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# MNIST dataset\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "image, _ = trainset[0]\n",
    "q_image = np.abs(image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Fashion-MNIST dataset\n",
    "trainset_fashion = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "image_fashion, _ = trainset_fashion[0]\n",
    "q_image_fashion = np.abs(image_fashion.numpy()[0])  # Extract the first Fashion-MNIST image\n",
    "\n",
    "# Download image (Pepper image)\n",
    "download_image = cv2.imread('C:/Users/HP/Desktop/pepper.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Check if the image is loaded correctly\n",
    "if download_image is None:\n",
    "    raise FileNotFoundError(\"The image file was not found. Please check the file path.\")\n",
    "\n",
    "download_image = cv2.resize(download_image, (28, 28))\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Parameters for quantum convolution\n",
    "pad_width = 2\n",
    "pool_size = 2\n",
    "m_BN = 0.9\n",
    "epsilon_BN = 1e-5\n",
    "\n",
    "# Different stride values to test\n",
    "stride_values = [1, 2, 3, 4, 5]  # Different stride sizes\n",
    "\n",
    "\n",
    "q_kernel = np.random.rand(28, 28) \n",
    "\n",
    "# Helper function to highlight changes\n",
    "def highlight_changes(original, processed):\n",
    "    difference = np.abs(processed - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))  # Create a blank RGB image\n",
    "    highlighter[..., 0] = difference  # Red channel for highlighting\n",
    "    highlighter[..., 1] = original  # Green channel for the original image intensity\n",
    "    highlighter[..., 2] = original  # Blue channel for the original image intensity\n",
    "    return highlighter\n",
    "\n",
    "# Plotting the results with clear pixel visibility after applying stride and convolution\n",
    "fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n",
    "\n",
    "# Apply quantum convolution to MNIST image with different stride values\n",
    "for i, stride in enumerate(stride_values):\n",
    "    q_conv_mnist = quantum_convolution(q_image, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN)\n",
    "    q_conv_mnist = np.clip(q_conv_mnist * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_high_res = resize(q_image, (28 * stride, 28 * stride), mode='reflect', anti_aliasing=False, order=0)  # Ensure pixel visibility\n",
    "    q_conv_high_res = resize(q_conv_mnist, (28 * stride, 28 * stride), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_mnist = highlight_changes(q_image_high_res, q_conv_high_res)\n",
    "    axs[0, i].imshow(highlighted_mnist, interpolation='nearest')  # Ensure each pixel is visible\n",
    "    axs[0, i].set_title(f'MNIST Stride {stride}')\n",
    "    axs[0, i].axis('off')\n",
    "\n",
    "# Apply quantum convolution to Fashion-MNIST image with different stride values\n",
    "for i, stride in enumerate(stride_values):\n",
    "    q_conv_fashion = quantum_convolution(q_image_fashion, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN)\n",
    "    q_conv_fashion = np.clip(q_conv_fashion * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_fashion_high_res = resize(q_image_fashion, (28 * stride, 28 * stride), mode='reflect', anti_aliasing=False, order=0)\n",
    "    q_conv_fashion_high_res = resize(q_conv_fashion, (28 * stride, 28 * stride), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_fashion = highlight_changes(q_image_fashion_high_res, q_conv_fashion_high_res)\n",
    "    axs[1, i].imshow(highlighted_fashion, interpolation='nearest')  # Ensure each pixel is visible\n",
    "    axs[1, i].set_title(f'Fashion-MNIST Stride {stride}')\n",
    "    axs[1, i].axis('off')\n",
    "\n",
    "# Apply quantum convolution to Download image with different stride values\n",
    "for i, stride in enumerate(stride_values):\n",
    "    q_conv_download = quantum_convolution(q_download_image, q_kernel, pad_width, pool_size, stride, m_BN, epsilon_BN)\n",
    "    q_conv_download = np.clip(q_conv_download * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_download_high_res = resize(q_download_image, (28 * stride, 28 * stride), mode='reflect', anti_aliasing=False, order=0)\n",
    "    q_conv_download_high_res = resize(q_conv_download, (28 * stride, 28 * stride), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_download = highlight_changes(q_image_download_high_res, q_conv_download_high_res)\n",
    "    axs[2, i].imshow(highlighted_download, interpolation='nearest')  # Ensure each pixel is visible\n",
    "    axs[2, i].set_title(f'Pepper Stride {stride}')\n",
    "    axs[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cdaf4-7bbf-4f19-96d3-2255639e8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Quantum Fourier Transform (QFT) and its inverse\n",
    "def quantum_fourier_transform(q_image):\n",
    "    return np.fft.fft2(q_image)\n",
    "\n",
    "def inverse_quantum_fourier_transform(q_image):\n",
    "    return np.fft.ifft2(q_image)\n",
    "\n",
    "# Padding operation\n",
    "def pad_image(q_image, pad_width):\n",
    "    return np.pad(q_image, pad_width, mode='constant')\n",
    "\n",
    "# Pooling operation (using average pooling)\n",
    "def quantum_pooling(q_image, pool_size):\n",
    "    output_shape = (q_image.shape[0] // pool_size, q_image.shape[1] // pool_size)\n",
    "    pooled_image = np.zeros(output_shape)\n",
    "    for i in range(output_shape[0]):\n",
    "        for j in range(output_shape[1]):\n",
    "            pooled_image[i, j] = np.mean(q_image[i*pool_size:(i+1)*pool_size, j*pool_size:(j+1)*pool_size])\n",
    "    return pooled_image\n",
    "\n",
    "# Batch normalization operation\n",
    "def batch_normalization(q_image, m_BN=0.9, epsilon_BN=1e-5):\n",
    "    mu = np.mean(q_image)\n",
    "    sigma2 = np.var(q_image)\n",
    "    return (q_image - mu) / np.sqrt(sigma2 + epsilon_BN)\n",
    "\n",
    "# Quantum convolution operation\n",
    "def quantum_convolution(q_image, q_kernel, pad_width, pool_size, m_BN, epsilon_BN):\n",
    "    # Padding\n",
    "    padded_image = pad_image(q_image, pad_width)\n",
    "    \n",
    "    # Quantum Fourier Transform\n",
    "    qft_image = quantum_fourier_transform(padded_image)\n",
    "    \n",
    "    # Adjust kernel size to match the padded image\n",
    "    kernel_padded = pad_image(q_kernel, ((padded_image.shape[0] - q_kernel.shape[0]) // 2, \n",
    "                                         (padded_image.shape[1] - q_kernel.shape[1]) // 2))\n",
    "    \n",
    "    # Convolution (element-wise multiplication in frequency domain)\n",
    "    conv_result = qft_image * kernel_padded\n",
    "    \n",
    "    # Inverse QFT\n",
    "    inv_qft_result = inverse_quantum_fourier_transform(conv_result)\n",
    "    \n",
    "    # Pooling\n",
    "    pooled_result = quantum_pooling(np.abs(inv_qft_result), pool_size)\n",
    "    \n",
    "    # Batch Normalization\n",
    "    normalized_result = batch_normalization(pooled_result, m_BN, epsilon_BN)\n",
    "    \n",
    "    return normalized_result\n",
    "\n",
    "# Load datasets\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# MNIST dataset\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "image, _ = trainset[0]\n",
    "q_image = np.abs(image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Fashion-MNIST dataset\n",
    "trainset_fashion = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "image_fashion, _ = trainset_fashion[0]\n",
    "q_image_fashion = np.abs(image_fashion.numpy()[0])  # Extract the first Fashion-MNIST image\n",
    "\n",
    "# Download image (Pepper image)\n",
    "download_image = cv2.imread('C:/Users/HP/Desktop/pepper.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Check if the image is loaded correctly\n",
    "if download_image is None:\n",
    "    raise FileNotFoundError(\"The image file was not found. Please check the file path.\")\n",
    "    \n",
    "download_image = cv2.resize(download_image, (28, 28))\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Parameters for quantum convolution\n",
    "pad_width = 2\n",
    "m_BN = 0.9\n",
    "epsilon_BN = 1e-5\n",
    "\n",
    "\n",
    "q_kernel = np.random.rand(28, 28)  # Same size as images\n",
    "\n",
    "# Different pooling sizes to test\n",
    "pool_sizes = [1, 2, 3, 4, 5]  # Different pool sizes\n",
    "\n",
    "# Helper function to highlight changes with clear pixel boundaries\n",
    "def highlight_changes(original, processed):\n",
    "    difference = np.abs(processed - original)\n",
    "    highlighter = np.zeros((*difference.shape, 3))  # Create a blank RGB image\n",
    "    highlighter[..., 0] = difference  # Red channel for highlighting\n",
    "    highlighter[..., 1] = original  # Green channel for the original image intensity\n",
    "    highlighter[..., 2] = original  # Blue channel for the original image intensity\n",
    "    return highlighter\n",
    "\n",
    "# Plotting the results with clearly visible pixels\n",
    "fig, axs = plt.subplots(3, 5, figsize=(20, 12))\n",
    "\n",
    "# Apply quantum convolution to MNIST image with different pooling sizes\n",
    "for i, pool_size in enumerate(pool_sizes):\n",
    "    q_conv_mnist = quantum_convolution(q_image, q_kernel, pad_width, pool_size, m_BN, epsilon_BN)\n",
    "    q_conv_mnist = np.clip(q_conv_mnist * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_squeezed = resize(q_image, (28, 28), mode='reflect', anti_aliasing=False, order=0)  # No smoothing, nearest neighbor interpolation\n",
    "    q_conv_squeezed = resize(q_conv_mnist, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_mnist = highlight_changes(q_image_squeezed, q_conv_squeezed)\n",
    "    axs[0, i].imshow(highlighted_mnist, interpolation='nearest')  # Ensure pixel clarity\n",
    "    axs[0, i].set_title(f'MNIST Pool Size {pool_size}')\n",
    "    axs[0, i].axis('off')\n",
    "\n",
    "# Apply quantum convolution to Fashion-MNIST image with different pooling sizes\n",
    "for i, pool_size in enumerate(pool_sizes):\n",
    "    q_conv_fashion = quantum_convolution(q_image_fashion, q_kernel, pad_width, pool_size, m_BN, epsilon_BN)\n",
    "    q_conv_fashion = np.clip(q_conv_fashion * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_fashion_squeezed = resize(q_image_fashion, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    q_conv_fashion_squeezed = resize(q_conv_fashion, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_fashion = highlight_changes(q_image_fashion_squeezed, q_conv_fashion_squeezed)\n",
    "    axs[1, i].imshow(highlighted_fashion, interpolation='nearest')  # Ensure pixel clarity\n",
    "    axs[1, i].set_title(f'Fashion-MNIST Pool Size {pool_size}')\n",
    "    axs[1, i].axis('off')\n",
    "\n",
    "# Apply quantum convolution to Download image with different pooling sizes\n",
    "for i, pool_size in enumerate(pool_sizes):\n",
    "    q_conv_download = quantum_convolution(q_download_image, q_kernel, pad_width, pool_size, m_BN, epsilon_BN)\n",
    "    q_conv_download = np.clip(q_conv_download * 2, 0, 1)  # Increase pixel intensity\n",
    "    q_image_download_squeezed = resize(q_download_image, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    q_conv_download_squeezed = resize(q_conv_download, (28, 28), mode='reflect', anti_aliasing=False, order=0)\n",
    "    highlighted_download = highlight_changes(q_image_download_squeezed, q_conv_download_squeezed)\n",
    "    axs[2, i].imshow(highlighted_download, interpolation='nearest')  # Ensure pixel clarity\n",
    "    axs[2, i].set_title(f'Pepper Pool Size {pool_size}')\n",
    "    axs[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133f3ed-b756-450f-8a38-b6fce566d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Added a third conv layer\n",
    "        # Use AdaptiveAvgPool2d to ensure a fixed output size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((8, 8))  # Reduces to 8x8 feature maps\n",
    "        self.fc1_input_size = 128 * 8 * 8  # Fixed size after adaptive pooling\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.MaxPool2d(2, 2)(x)\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.MaxPool2d(2, 2)(x)\n",
    "        x = nn.ReLU()(self.conv3(x))\n",
    "        x = nn.MaxPool2d(2, 2)(x)  # Added a third pooling layer\n",
    "\n",
    "        # Apply adaptive pooling to make the output size fixed\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Custom dataset class for single images like Pepper and Liver\n",
    "# Custom dataset class for single images like Pepper and Liver\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image, label):\n",
    "        self.image = image\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Only one image in this custom dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Resize the image to (1, 1024, 1024) to match the CNN's input expectations\n",
    "        image = torch.tensor(self.image, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        label = torch.tensor(self.label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "mnist_train_loader = DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_testset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_testset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=False, transform=transform)\n",
    "fashion_train_loader = DataLoader(fashion_trainset, batch_size=64, shuffle=True)\n",
    "fashion_test_loader = DataLoader(fashion_testset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Load Pepper image\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (1024, 1024))  # Resize to 1024x1024\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (1024, 1024))  # Resize to 1024x1024\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Custom loaders for Pepper and Liver images\n",
    "pepper_dataset = CustomImageDataset(q_download_image, 0)  # Assuming label 0 for Pepper\n",
    "pepper_loader = DataLoader(pepper_dataset, batch_size=1, shuffle=False)\n",
    "liver_dataset = CustomImageDataset(q_liver_image, 1)  # Assuming label 1 for Liver\n",
    "liver_loader = DataLoader(liver_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(target.numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Function to compute performance metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Initialize model, criterion, and optimizer for MNIST\n",
    "model_mnist = SimpleCNN(num_classes=10)  # MNIST has 10 classes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mnist.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model on MNIST\n",
    "start_train_time = time.time()\n",
    "train_model(model_mnist, mnist_train_loader, criterion, optimizer, epochs=1)  # Use more epochs for better accuracy\n",
    "end_train_time = time.time()\n",
    "\n",
    "# Evaluate on MNIST test set\n",
    "start_inference_time = time.time()\n",
    "mnist_preds, mnist_labels = evaluate_model(model_mnist, mnist_test_loader)\n",
    "end_inference_time = time.time()\n",
    "\n",
    "# Compute metrics for MNIST\n",
    "mnist_accuracy, mnist_precision, mnist_recall, mnist_f1 = compute_metrics(mnist_labels, mnist_preds)\n",
    "mnist_train_time = end_train_time - start_train_time\n",
    "mnist_inference_time = end_inference_time - start_inference_time\n",
    "\n",
    "# Display MNIST results\n",
    "print(f\"MNIST - Accuracy: {mnist_accuracy}, Precision: {mnist_precision}, Recall: {mnist_recall}, F1 Score: {mnist_f1}\")\n",
    "print(f\"MNIST - Training Time: {mnist_train_time}s, Inference Time: {mnist_inference_time}s\")\n",
    "\n",
    "# For Pepper and Liver, we can use a different model architecture if desired\n",
    "model_pepper_liver = SimpleCNN(num_classes=2)  # Adjust for binary classification\n",
    "# You might need to resize input to match model requirements or change the model architecture\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_pepper_liver.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model on Pepper (not really possible with a single image, so we just use the model directly for evaluation)\n",
    "# Evaluate on Pepper image\n",
    "start_inference_time = time.time()\n",
    "pepper_preds, pepper_labels = evaluate_model(model_pepper_liver, pepper_loader)\n",
    "end_inference_time = time.time()\n",
    "\n",
    "# Compute metrics for Pepper\n",
    "pepper_accuracy, pepper_precision, pepper_recall, pepper_f1 = compute_metrics([0], pepper_preds)  # True label is 0\n",
    "pepper_inference_time = end_inference_time - start_inference_time\n",
    "\n",
    "# Display Pepper results\n",
    "print(f\"Pepper - Accuracy: {pepper_accuracy}, Precision: {pepper_precision}, Recall: {pepper_recall}, F1 Score: {pepper_f1}\")\n",
    "print(f\"Pepper - Inference Time: {pepper_inference_time}s\")\n",
    "\n",
    "# Repeat similarly for Liver image (assume true label is 1)\n",
    "start_inference_time = time.time()\n",
    "liver_preds, liver_labels = evaluate_model(model_pepper_liver, liver_loader)\n",
    "end_inference_time = time.time()\n",
    "\n",
    "# Compute metrics for Liver\n",
    "liver_accuracy, liver_precision, liver_recall, liver_f1 = compute_metrics([1], liver_preds)  # True label is 1\n",
    "liver_inference_time = end_inference_time - start_inference_time\n",
    "\n",
    "# Display Liver results\n",
    "print(f\"Liver - Accuracy: {liver_accuracy}, Precision: {liver_precision}, Recall: {liver_recall}, F1 Score: {liver_f1}\")\n",
    "print(f\"Liver - Inference Time: {liver_inference_time}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb2e0b-4051-422b-a0c1-28647eec1c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transformation for MNIST and FashionMNIST datasets\n",
    "transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "mnist_image, _ = mnist_trainset[0]\n",
    "q_mnist_image = np.abs(mnist_image.numpy()[0])  # Extract the first MNIST image\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "fashion_trainset = datasets.FashionMNIST(root='~/.pytorch/FashionMNIST_data/', download=True, train=True, transform=transform)\n",
    "fashion_image, _ = fashion_trainset[0]\n",
    "q_fashion_image = np.abs(fashion_image.numpy()[0])  # Extract the first FashionMNIST image\n",
    "\n",
    "# Load Pepper image\n",
    "download_image_path = 'C:/Users/HP/Desktop/pepper.jpg'\n",
    "download_image = cv2.imread(download_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if download_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {download_image_path}\")\n",
    "download_image = cv2.resize(download_image, (256, 256))  # Increase resolution\n",
    "q_download_image = download_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Load Liver image\n",
    "liver_image_path = 'C:/Users/HP/Desktop/3Dircadb1.11/3Dircadb1.11/liver_11.jpg'\n",
    "liver_image = cv2.imread(liver_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "if liver_image is None:\n",
    "    raise ValueError(f\"Failed to load image from path: {liver_image_path}\")\n",
    "liver_image = cv2.resize(liver_image, (256, 256))  # Increase resolution\n",
    "q_liver_image = liver_image / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define Optimized Convolution Kernels (Extended)\n",
    "kernels = [\n",
    "    np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),  # Sharpen Kernel\n",
    "    np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),  # Edge Detection\n",
    "    np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]),  # Sobel X Kernel\n",
    "    np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]]),  # Sobel Y Kernel\n",
    "    np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]]),  # Laplacian Kernel\n",
    "]\n",
    "\n",
    "# Apply convolution and pooling\n",
    "def quantum_convolution(image, kernels):\n",
    "    convolved_images = []\n",
    "    for kernel in kernels:\n",
    "        convolved = cv2.filter2D(image, -1, kernel)\n",
    "        pooled = cv2.resize(convolved, (128, 128))  # Max pooling simulation by resizing to 128x128\n",
    "        convolved_images.append(pooled)\n",
    "    return convolved_images\n",
    "\n",
    "# Process all images\n",
    "images = [q_mnist_image, q_fashion_image, q_download_image, q_liver_image]\n",
    "titles = ['MNIST Image', 'FashionMNIST Image', 'Pepper Image', 'Liver Image']\n",
    "\n",
    "# Optimize and Visualize\n",
    "plt.figure(figsize=(20, 18))  # Increased figure size for better readability\n",
    "for idx, image in enumerate(images):\n",
    "    convolved_images = quantum_convolution(image, kernels)\n",
    "    for k_idx, convolved_image in enumerate(convolved_images):\n",
    "        ax = plt.subplot(len(images), len(kernels), idx * len(kernels) + k_idx + 1)\n",
    "        plt.imshow(convolved_image, cmap='inferno')\n",
    "        plt.title(f'{titles[idx]} - Kernel {k_idx + 1}', fontsize=10)  # Highlight titles\n",
    "        plt.axis('off')\n",
    "        ax.set_facecolor('xkcd:light grey')  # Highlight each subplot background\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
